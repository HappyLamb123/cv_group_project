<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Gatech_CV_Cacao</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="assets/"> -->

  <!-- Video playback speed and no controls -->
  <script defer src="js/video.js"></script>
</head>

<body>
  <div class="container">


        </br>

        <h3 style="text-align: center;">Bridging from images to actions : </br>How computer vision techniques affect RL agent? </h3>
        <h5 style="text-align: center;">Georgia Tech CS6476 Project (Mar 2024)  </br>Eva Li, Juntao He, and Seungeun Rho</h5>


    </div>


  <!-- Companies
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<!--  <div class="container">-->
<!--    <div class="div_line"></div><br>-->
<!--    <div class="row">-->
<!--      <div class="four columns center">-->
<!--        <a href="https://www.cc.gatech.edu/">-->
<!--          <img src="assets/icons/org/Computing.png" alt="CS department Logo" class="logo">-->
<!--        </a>-->
<!--        &lt;!&ndash; <p class="company-name">-->
<!--          Research Intern @ The AI Institute<br>2024 - Present-->
<!--        </p> &ndash;&gt;-->
<!--      </div>-->
<!--      <div class="four columns center">-->
<!--        <a href="https://www.gatech.edu/">-->
<!--          <img src="assets/icons/org/GeorgiaTechLogo.png" alt="Georgia Tech Logo" class="logo">-->
<!--        </a>-->
<!--        &lt;!&ndash; <p class="company-name">-->
<!--          PhD in Robotics @ Georgia Tech<br>2020 - Present-->
<!--        </p> &ndash;&gt;-->
<!--      </div>-->
<!--      <div class="four columns center">-->
<!--        <div class="row">-->
<!--          <a href="https://research.gatech.edu/robotics">-->
<!--            <img src="assets/icons/org/GA-Tech-IRIM-logo.png" alt="IRIM Logo" class="logo">-->
<!--          </a>-->
<!--        </div>-->
<!--        &lt;!&ndash; <p class="company-name">-->
<!--          AI Residency @ Google X<br>2021 - 2022-->
<!--        </p> &ndash;&gt;-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--  -->
  <!-- Latest
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Project description </u></h5>

    <figure>

      <img src="assets/img/fig1.png" alt=" fig1" width="200" class="center">
      <figcaption class="center">Figure 1. An in-game scene of Atari-Breakout</figcaption>

    </figure>
    <p>
      <h6><u>High Level Description and Motivation:</u></h6>
      Our project sits at the crossroads of computer vision and reinforcement learning (RL), with a goal to enhance the perceptual capabilities of RL agents in game environments. Our motivation for the project is to improve agents’ ability to utilize visual information to make informed decisions, a capability crucial for applications like autonomous navigation and interactive entertainment.</p>
    <p>
      <h6><u>Specific Problem Definition:</u></h6>
      Specifically, we focus on using the Breakout game to evaluate how various image preprocessing techniques like Edge Detector or Vision Transformer can improve RL performance. We believe with enhanced edge information the agent should be able to capture useful structural information more quickly and make more informed decision in quicker manner. With Vision Transformer which treats an image as a sequence of patches and applies self-attention mechanisms we also expect it to perform better than baseline image-based RL. Additionally, we also explored whether adding auxiliary loss information would help agent be better informed of current environmental information and in turn improve performance.
      </br></br>
    </p>


     <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <!-- <div class="row video-row"> -->
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <!-- <div class="four columns">
        <div class="video-container">
          <video id="learning-robot" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/learning_robot_morph.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column"> -->
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <div class="project_title">On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and
          Learning</div> -->
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <br><em><strong>Maks Sorokin</strong>, Chuyuan Fu, Jie Tan, C. Karen Liu, Yunfei Bai, Wenlong Lu, Sehoon Ha,
          Mohi Khansari</em>
        <br><em><span class="venue">International Conference on Intelligent Robots and Systems (IROS) 2023</span></em> -->
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <p class="project_info">
          We present a learning-oriented morphology optimization framework that accounts for the interplay between the
          robot's morphology, onboard perception abilities, and their interaction in different tasks.
          We find that morphologies optimized holistically improve the robot performance by 15-20% on
          various manipulation tasks, and require 25x less data to match human-expert made morphology performance.
        </p> -->
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <a href="https://learning-robot.github.io/">[project page]</a>
        <a href="https://www.youtube.com/watch?v=w9B0COjGvfo">[video overview]</a>
        <a href="https://arxiv.org/pdf/2303.13390.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2303.13390">[arXiv]</a>
      </div>
    </div> -->

  </div>

  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Related Works</u></h5>
    In this section, we cover representative works from the domain of image based deep RL.
    <p>
    <li><b>Schulman, John, et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).</b></li>
      The paper introduces the Proximal Policy Optimization (PPO) algorithm, a family of policy gradient methods for reinforcement learning.
      To train and evaluate the performance for each of methods, we used this algorithm. This paper demonstrates the effectiveness of PPO algorithm in various
      benchmark tasks, including simulated robotic locomotion and Atari game playing. Specifically, the paper compares PPO with other algorithms on the Atari domain,
      showcasing its superior performance in terms of sample complexity. In the Breakout environment, PPO demonstrates significant improvements in learning
      efficiency compared to other algorithms, highlighting its potential for achieving stable and reliable policy optimization in image-based reinforcement learning
       tasks.

    </br></br>
    <li><b>Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).</b></li>
    This paper presents the use of transformer for encoding images. Before this paper came out, transformer was mostly used for encoding sequential data.
    This paper suggests that you can translate image into smaller patches and flatten in 1D to make it look like sequential data, and using transformer
    on top of it gives us efficient embeddings of image.
    </br></br>
    <li><b>D. Kalashnikov et al., "Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation," in Proceedings of The 2nd Conference on Robot
      Learning, vol. 87, pp. 651-673, PMLR, 2018 </b></li>
      The QT-Opt algorithm presented in this paper is relevant to image-based reinforcement learning for environments like Atari Breakout.
      Like Atari games, QT-Opt learns policies directly from raw pixel inputs using deep Q-learning.
      The distributed off-policy training procedure enables learning from massive amounts of experience data, which could potentially be useful for increasing the sample efficiency
      for gaming enviromnets such as Atari.
      Additionally, the stochastic optimization used in QT-Opt to select actions could help with the high-dimensional and potentially multimodal action spaces present
      in many Atari games.
    </br></br>

      <li><b>D. Yarats, et al. "Reinforcement Learning with Prototypical Representations," in Proceedings of the 38th International Conference on Machine Learning,
        vol. 139, pp. 11920-11931, PMLR, 2021. </b></li>
      Proto-RL is a novel self-supervised framework that learns visual representations and prototypes in a task-agnostic pre-training phase for image-based reinforcement learning. 
      During pre-training, it explores the environment to maximize state entropy while learning the representations/prototypes. 
      The learned representations and prototypes then accelerate downstream task learning by providing effective embeddings and enabling better exploration, leading to state-of-the-art performance on various challenging visual control tasks from the DeepMind Control Suite.
    </br></br>

    <li><b>Huang, Yangru, et al. "Spectrum random masking for generalization in image-based reinforcement learning." Advances in Neural Information Processing
      Systems 35 (2022): 20393-20406.</b></li>
      The paper discusses the challenge of generalization in image-based reinforcement learning (RL) and proposes a novel augmentation method called Spectrum Random Masking (SRM) to address this issue. 
      The proposed SRM method's focus on frequency-based augmentation and its potential to improve generalization aligns with the challenges faced in training RL agents for Atari games, where visual inputs and distribution shifts are significant factors impacting performance.
    </br></br>
      While the mentioned studies have significantly advanced the field of RL in the context of visual environments, there remains a gap in understanding how different image preprocessing techniques, such as edge detection, and neural network architectures specifically influence RL agent performance. Our work aims to address this by conducting a comparative study of preprocessing methods and neural architectures like CNNs and ViTs. Furthermore, we propose the integration of auxiliary loss functions to enhance learning signals, a dimension not fully explored in the context of sparse reward environments typical of many RL scenarios.
    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Methods/Approach</u></h5>
    <p>
      <h6><u> Methods Overview</u></h6>
      Our objective is to train an RL agent using the Proximal Policy Optimization (PPO) algorithm, and see how the performance varies depending on different types of Computer Vision(CV) techniques. As stated in the first section of the report, we explored three different approaches to achieve our goal.
    </p>  
      <ol>
        <li>
          <strong>Enhancing observation using Edge Detectors</strong> By augmenting the raw visual data with edge information, we aim to provide the RL agent with more salient features to better understand the game dynamics. Our primary goal is to evaluate whether incorporating edge detection preprocessing enhances the training performance of the RL agent, ultimately leading to improved gameplay proficiency.
        </li>
        <li>
          <strong>Ablation study of neural network architectures - CNN vs Vision Transformer</strong> During the coursework we learned two most prominent neural network archiectures for image encoding; Convolutional Neural Networks(CNN) and Vision Transformer(ViT). We aim to compare the effectiveness of each architecture for encoding image for RL agents.
        </li>
        <li>
          <strong>Integrating Auxiliary Loss for Reward Prediction</strong> We introduce an auxiliary loss function aimed at predicting the immediate reward based on the current state's image. This predictive task is expected to provide the agent with a richer learning signal, potentially enhancing the learning process, especially in the early stages when the reward signal is more sparse. We will assess whether this additional learning objective facilitates a quicker and more robust convergence of the agent's policy.
        </li>
      </ol>
      <figure>

        <img src="assets/img/fig 8 flow chart.png" alt=" fig2" width="600" class="center">
        <figcaption class="center">Figure 2. Approach Pipeline</figcaption>
  
      </figure>
    



    <div class="eleven columns ">
      <h5><u>Experiment Setup </u></h5>
    <p>Our objective is to train an RL agent using the Proximal Policy Optimization (PPO) algorithm, and see how the performance varies depending on differnet types of Computer Vision(CV) techniques. 
      We directly implemented RL algorihtm, Proximal Policy Optimization (PPO) by ourselves to avoid the overhead of more complex systems. This enhanced computational efficiency and facilitates experimentation.
      </p>  
      
    </div>
    <div class="eleven columns ">
    We directly implemented RL algorihtm, Proximal Policy Optimization (PPO) by ourselves to avoid the overhead of more complex systems. This enhanced computational
      efficiency and facilitates experimentation.

    <ul>
      <li><strong>Convolutional Neural Network (CNN):</strong> Our model architecture includes a CNN tailored for extracting features from game frames. The network consists of three convolutional layers, each followed by ReLU activation and max pooling, culminating in a flattened layer for further processing.</li>
      <li><strong>Policy and Value Networks:</strong> Separate networks derive from the CNN's output, with the policy network predicting action probabilities and the value network estimating state values. This separation facilitates the efficient optimization of both policy and value predictions.</li>
    </ul>

    <p>Our training process is governed by a carefully selected set of hyperparameters, optimized to balance learning efficiency and performance:</p>

    <ul>
      <li><strong>Learning Rate:</strong> 0.0001, controls the step size during optimization, ensuring steady convergence without overshooting.</li>
      <li><strong>Gamma (γ):</strong> 0.98, discounts future rewards, balancing immediate and long-term gains.</li>
      <li><strong>Lambda (λ):</strong> 0.95, used in the Generalized Advantage Estimation (GAE), influencing the bias-variance trade-off in advantage computation.</li>
      <li><strong>Epsilon Clip (ε):</strong> 0.1, limits the ratio of new to old policy probabilities, preventing large policy updates that could destabilize training.</li>
      <li><strong>K Epochs:</strong> 3, the number of epochs for updating the policy using the same set of sampled data, ensuring thorough utilization of each batch.</li>
      <li><strong>T Horizon:</strong> 20, defines the length of the trajectory segment used for each update, balancing the trade-off between update frequency and the quality of the advantage estimates.</li>
    </ul>
    When evaluating for edge detector’s performance a total of 45k steps were taken. When evaluating for auxiliary loss added model’s performance a total of 100k steps were taken. The performance of the model is evaluated based on the score the agent was able to achieve as the training steps increase.
  </br></br>
<p><strong>Custom Vector Environment</strong></p>

<p>We also implemented designed algorithm in vectorized environment so that multiple instances of the environment can be run in parallel. This means that the agent can collect more diverse experience in a shorter amount of time, leading to faster learning and improved overall performance.</p>
<p>First techniques is classical; we are going to apply edge detector for enhancing raw observation, and see whether it gives more avid signals when the scene is changed during the game. </p>
<p><b>1) Enhancing observation using Edge Detectors </b></p>
      <figure>
          <img src="assets/img/fig7.png" alt=" fig1" width="1000" class="center">
          <figcaption class="center">Figure 3. Frame Inputs to the Model</figcaption>
        </figure>
        <p>A pivotal aspect of our methodology is the application of targeted image preprocessing techniques aimed at optimizing the visual information fed into
          the model.To optimize the agent's ability to learn from visual inputs, we developed a preprocessing pipeline that includes the following
          steps. First, the original input observations from the game is RGB image with size of 210 x 160 (Figure 3.a). We first <b>preprocess</b> this image and then applies
          edge detectors on top of it. Figure 3 illustrates the examplary image from each procedure
          <ul>
            <li>(a) Original Frame</li>
            <li>(b) After Preprocessing</li>
              <ul>
                  <li>resize into 84 x 84</li>
                  <li>transform into gray image</li>
                  <li>cut out the top 16 pixels and bottom 4 pixels which are irrelevant to game playing</li>
                  <li>stack 4 consecutive images :  providing the agent with temporal context necessary for understanding motion and progression within the game.</li>
                </ul>
            <li>(c) Edge Enhanced Frame After Preprocessing </li>
            <li>(d) Edge Enhanced + Color Scale Frame After Preprocessing</li>
          </ul>
<p><b>2) Ablation study of neural network architectures - CNN vs Vision Transformer </b></p>
##########################PLACEHOLDER#####################
<p><b>3) Integrating Auxiliary Loss for Reward Prediction </b></p>
<p>An auxiliary loss component is added if configured, which uses the reward predictions as a form of additional supervision to improve learning. The reward predictions use the CNN to process the input observation and produce a feature representation. This auxiliary loss is integrated into the overall loss function to enhance the prediction accuracy of the rewards based on the agent's actions and states. </p>


</div>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Results</u></h5>
    <h6><b>1) Enhancing observation using Edge Detectors</b></h6>
    <p>We conducted three distinct training experiments on the Breakout game, employing different image preprocessing strategies to evaluate their impact on learning performance (Figure 4).</p>
<ol>
  <li>
    <strong>Default Preprocessing (Blue):</strong> Our first experiment adhered to the default preprocessing settings provided by OpenAI's AtariPreprocessing, FrameStacking, and simple crop of top and bottom of the page that consist of useless information for agent training. This method provided a baseline for performance comparison, focusing on the original visual representation with minimal modifications. Shows a score ending near 40.
  </li>
  <li>
    <strong>Edge Detection Only (Black):</strong> The second approach used edge detection exclusively to process the game images. This technique, while effective in highlighting structural information, resulted in suboptimal performance due to the absence of color data, which is essential for recognizing score-related changes in the game environment. Shows a score increase modestly to just above 20.
  </li>
  <li>
    <strong>Edge Detection with Image Stacking (Pink):</strong> The most successful approach combined edge detection with image stacking, integrating both structural and temporal information into the AI's decision-making process with scores improving significantly to reach around 60. This method significantly outperformed the others, indicating that a composite representation of visual data, incorporating both immediate and historical game states, yields superior learning outcomes.
  </li>
</ol>
<style>  .custom-figure {
  width: 800px; /* Adjust width as needed */
  height: auto; /* Adjust height as needed, or use a fixed height */
  float: left;
  margin-right: 20px; /* Optional: Add some spacing between the figure and other content */
}
  /* Additional CSS for demonstration purposes */
  .custom-figure img {
    max-width: 100%; /* Ensure the image doesn't exceed the figure width */
    height: auto; /* Maintain aspect ratio */
    display: block; /* Ensure proper alignment */
    margin: 0 auto; /* Center the image horizontally */
  }

</style>

<figure >
  <img src="assets/img/fig5_new.png" alt=" fig5" class="center" width="1000">
  <figcaption class="center">Fig 4. Score Change of Three Preprocessing Procedures Over 45k Training Iterations</figcaption>
</figure>

  <h6><b>2) Ablation study of neural network architectures - CNN vs Vision Transformer</b></h6>
  ##########################PLACEHOLDER#####################
  <h6><b>3) Integrating Auxiliary Loss for Reward Prediction</b></h6>
<ol>
<li>
  <strong>Standard PPO (Green): </strong> The baseline method follows traditional reinforcement learning practices. Initially, the learning progress appeared gradual, and towards the end, it achieved a stable but lower performance, ending near a score of 120.
</li>
<li>
  <strong>PPO with Auxiliary Loss (Gray Curve): </strong> This method initially showed a significant boost in performance during the early training phase, suggesting that the auxiliary loss was effective in providing a more informative signal to guide the learning process. However, as training progressed, the model with auxiliary loss was eventually outperformed by the standard PPO model. This indicates that while auxiliary loss can accelerate early learning, it may lead to suboptimal policy refinement in later stages
</li>
</ol>
<p>The results indicate that the auxiliary loss has a pronounced impact on the initial training phase, likely due to its role in sharpening the reward signal and thus guiding the agent to quicker preliminary learning. However, over extended training iterations, the advantage of auxiliary loss diminishes, and the standard PPO model without such additional loss overtakes in performance, suggesting that auxiliary loss may introduce some limitations to long-term policy optimization.</p>
<figure >
  <img src="assets/img/fig9.png" alt=" fig9" class="center" width="600">
  <figcaption class="center">Fig 5.Score Change of Model with Auxiliary Loss of 100k Training Iterations</figcaption>
</figure>
</div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Discussion</u></h5>
    <p>
      In this project our aim was to explore how various image preprocessing techniques could potentially improve an agent's ability to utilize visual information for decision-making.
    </br></br>
    Our approach involved experimenting with methods like Edge Detection and Vision Transformers (ViT) to determine their impact on the performance of RL agents. We hypothesized that by leveraging the structural information provided by these techniques, the RL agent could make quicker and more informed decisions. Indeed, through our experiments with the Breakout game, we found that a composite of edge detection and image stacking markedly improved performance, suggesting the value of combining structural and temporal visual cues.
    </br></br>
    Additionally, our exploration into the effect of auxiliary loss information for better informing the agent of its environmental state yielded intriguing results. While the auxiliary loss accelerated initial learning, its benefit tapered off, and the agent trained with a standard PPO eventually surpassed it in performance.
    </br></br>
    For future work, several avenues appear promising. First, refining the integration of auxiliary loss to sustain its benefits throughout the learning process could be explored. Second, investigating other hybrid neural architectures that combine the strengths of CNNs and ViTs for image-based RL tasks could potentially yield improvements in agent performance.
    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Challenges</u></h5>
    <p>
      One challenge we encountered at the beginning of the project was we misunderstood the rule for Breakout game and had the expectation that the model would be able to perform well with pure edge enhanced graph. We neglected the rule emphasizing the association between score and color thus wasted time in training model that was obvious not going to success. 
    </br></br>
      One other significant challenge was fine-tuning the preprocessing methods to enhance the RL agent's perception without overwhelming it with redundant or irrelevant information. 
    </br></br>
      We also struggled with balancing the computational efficiency against the complexity of the models, particularly when implementing the Vision Transformers, which are inherently more resource-intensive than simpler architectures like CNNs.
    </br></br>
    If we were to start the project anew, we would likely allocate more time for experimenting with different neural network architectures, potentially considering lightweight variations of Vision Transformers that retain their effectiveness while being less computationally demanding. Additionally, we would explore more sophisticated methods of feature extraction that could provide more nuanced visual cues to the RL agent without significantly increasing the computational load. This could involve the use of advanced edge detection techniques that focus on dynamic changes within the game environment, which could be more informative for the agent's decision-making processes.
    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Team member contributions</u></h5>
    <li>Eva Li : Implemented Edge detector and experiments. Tested different types of methods and report results. Result anlysis and report writing. </li>

    <li>Seungeun Rho : In charge of implementations. Implemented backbone PPO codes, custom Vectorized environments, agent architectures, and training code. Implemented Auxiliary Loss and experiments. Implemented VIT and experiments. Report writing.</li>

    <li>Juntao He : Project webpage and report. Implemented VIT and experimented. Related work research. Also helped implementing features from backbone codes.</li>

  
  <!-- Teaching ––––––––––––––––––––––––––––––––––––––––––––––––––
  <div class="container">
    <div class="div_line"></div><br><br>
    <div class="row profile-row">
      <div class="one columns hide-on-small">&nbsp;</div>
      <div class="nine columns ">
        <h5><u>Mentoring Experience</u></h5>
        I've had a great pleasure working with a number of exceptional students at Georgia Tech.<br><br>
        <ul>
          <li><strong>PRESENT</strong>: Master's Student - <a href="https://jxu443.github.io/portfolio/">Jiaxi Xu</a></li>
          <li><strong>FALL 2021</strong>: Master's Student - <a href="https://arjun-krishna.github.io/">Arjun Krishna</a> -> PhD student at UPenn’s GRASP lab </li>
          <li><strong>FALL 2020</strong>: Master's Student - <a href="https://qianluo.netlify.app">Qian Luo</a> ->
            NLP Algorithm Engineer at Alibaba Group</li>
        </ul>
        <br>
        <h5><u>Teaching Experience</u></h5>
        I had an amazing experience helping teach one of the largest classes (1000+ students) at Georgia Tech.
        <br>CS6601 - Artificial Intelligence class by
        <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Dr. Thomas Ploetz</a> & <a
          href="https://www.cc.gatech.edu/home/thad/">Dr. Thad Starner</a>.
        <br><br>
        <ul>
          <li><strong>FALL 2019 & SPRING 2020</strong>: Head Teaching Assistant </li>
          <li><strong>FALL 2018 & SPRING 2019</strong>: Teaching Assistant </li>
        </ul>
        <br>
        <h5><u>Scholarly Activities</u></h5>
        <ul>
          <li><strong>IROS 2023</strong> - Session co-chair Mechanism Design </li>
          <li><strong>RA-L 2023</strong> - Reviewer at IEEE Robotics and Automation Letters </li>
          <li><strong>RSS 2023</strong> - Reviewer at Proceedings of Robotics: Science and Systems </li>
          <li><strong>RA-L 2022</strong> - Reviewer at IEEE Robotics and Automation Letters</li>
          <li><strong>ICRA 2021</strong> - Reviewer at IEEE International Conference on Robotics and Automation</li>
        </ul>
      </div>
    </div>
  </div> -->


  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- attempt hiding from spambots :p - snippet credits Andrej Karpathy - karpathy.ai source code -->
  <script type="text/javascript">
    var e_is_shown = false;
    document.getElementById('iemail').addEventListener("click", function () {
      let demail = document.getElementById('demail');
      demail.innerHTML = 'm' + 'aks' + ' _a' + 't_ ' + 'gatech' + '.' + 'e' + 'du';
      demail.style.opacity = e_is_shown ? 0 : 1;
      e_is_shown = !e_is_shown;
    })
  </script>


  <!-- ### FOOTER ### -->

  <!-- <br>
  <br>

  <br>
  <br>
  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    <h7 style="text-decoration: none;">consider checking out: </h7><br><br>
    <a href="https://www.givewell.org/"><img src="assets/icons/gw.jpg" style="width:100px;"></a>
    <a href="https://www.givingwhatwecan.org/"><img src="assets/icons/gwwc.png" style="width:100px;"></a>
    <a href="https://www.effectivealtruism.org/"><img src="assets/icons/ea.png" style="width:80px; margin:10px"></a>

  </p>


  <div class="div_line"></div><br> -->

  <!-- <p style="color:gray; text-align:center; font-size: small;">
    2023© Maks Sorokin<br>
    built using <a href="http://getskeleton.com/">Skeleton</a>,
    icon credits <a href="https://www.flaticon.com/"> flaticon</a>,
    hosted by <a href="https://pages.github.com/"> GitHub Pages</a>❤️
    <br>
    <br>
    feel free to copy: <a href="https://github.com/initmaks/initmaks.github.io">this page</a>
  </p> -->

</body>

</html>