<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="icon" href="assets/icons/icon.png">
  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Gatech_CV_Cacao</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="assets/"> -->

  <!-- Video playback speed and no controls -->
  <script defer src="js/video.js"></script>
</head>

<body>
  <div class="container">
    <!-- NAVBAR --------------------------------- -->
    <!-- <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="index.html">About</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/blog.html">Blog</a></li>
        </ul>
      </div>
    </nav> -->

    <!-- Personal Info  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row profile-row" style="margin-top: 4%">
      <div class="one columns"></div>
      <div class="three columns">
        <!-- Profile photo  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div style="text-align: center;">
          <img src="assets/img/photo.webp" class="profile-photo" alt="profile photo"><br>
        </div>
        <!-- Name
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <h5 style="text-align: center;">Gatech_CV_Cacao</h5>
        <!-- Social links
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <div id="social">
          <a href="https://twitter.com/initmaks"><img src="assets/icons/twitter.png" class="iico" /></a>
          <a rel="me" href="https://sigmoid.social/@maks"><img src="assets/icons/mastodon.png" class="iico" /></a>
          <a href="https://github.com/initmaks"><img src="assets/icons/github.png" class="iico" /></a>
          <a href="assets/pdfs/CV.pdf"><img src="assets/icons/file.png" class="iico" /></a>
          <img src="assets/icons/mail.png" style="cursor: pointer;" class="iico" id="iemail" title="click to reveal" />
        </div> -->
        <!-- <div id="demail"></div> will reveal email -->
      </div>

      <div class="eight columns" style="margin-top: 5%;">
        <p style="text-align:justify;">
          <!-- Self-introduction
          –––––––––––––––––––––––––––––––––––––––––––––––––– -->
          This webpage contains the  materials for the CS 6476 group project. Team members are Seungeun Rho, Eva Li and Juntao He.
        </p>
        <!-- <strong>Competences:</strong>
        <a>python</a>,
        <a>pytorch</a>,
        <a>pybullet</a>,
        <a>iGibson</a>,
        <a>OpenCV</a>,
        <a>numpy</a>,
        <a>Tensorflow</a>,
        <a>C/C++</a>,
        <a>ROS</a>,
        <a>docker</a> -->

        <h5><u>Project description</u></h5>
        <ul>
          <p>In this project, we embark on a journey into the realm of Reinforcement Learning (RL) by conducting experiments in the Atari gaming domain, with a particular focus on the Breakout environment. 
            Our objective is to train an RL agent using the Proximal Policy Optimization (PPO) algorithm, a renowned technique for its stability and effectiveness in training deep RL agents. 
            The input observations provided to the agent consist of 84 x 84 grayscale images capturing the game screen at each timestep.

            A novel aspect of our approach involves modifying the input observations using edge detection techniques.
             By augmenting the raw visual data with edge information, we aim to provide the RL agent with more salient features to better understand the game dynamics. Our primary goal is to evaluate whether incorporating edge detection preprocessing enhances the training performance of the RL agent, ultimately leading to improved gameplay proficiency. Through rigorous experimentation and analysis, we seek to uncover insights into the interplay between image preprocessing techniques and RL training dynamics, thereby contributing to advancements in both gaming agent development and computer vision research.</p>
          <!-- <li> <b>JUL'23</b> - Our work on Learning-oriented Robot Design was accepted at IROS 2023 <a
            href="https://learning-robot.github.io/">[project page]</a></li>
          <li> <b>MAR'23</b> - Check out our latest work on Design a Learning Robot <a
              href="https://learning-robot.github.io/">[project page]</a></li>
          <li> <b>APR'22</b> - Human Motion Control of Quadrupedal Robots accepted at RSS <a
              href="https://sites.google.com/view/humanconquad/">[project page]</a></li>
          <li> <b>SEP'21</b> - Excited to be joining <a href="https://x.company">X - the moonshot factory</a> ( <a
              href="https://everydayrobots.com">Everyday Robots</a> ) for PhD
            Residency!</li>
          <li> <b>SEP'21</b> - Check out our work on Learning Sidewalk Navigation <a
              href="./navigation.html">[project page]</a></li> -->
          <!-- <li> Add details here</li> -->
          <!-- <li> <b>MAY'21</b> - Awarded the fellowship by the Machine Learning Center at Georgia Tech. <a href="https://mlatgt.blog/2021/05/10/the-machine-learning-center-awards-inaugural-mlgt-fellows/">[link]</a> </li> -->
          <!-- <li> <b>FEB'21</b> Paper on Learning Human Search Behavior accepted at EUROGRAPHICS'2021! <a href="https://arxiv.org/pdf/2011.03618.pdf">[pdf]</a><a href="https://arxiv.org/abs/2011.03618">[arXiv]</a></li> -->
          <!-- <li> <b>DEC'20</b> Paper on Few-shot visual sensor meta-adaptation accepted at ICRA'2021! <a href="https://arxiv.org/pdf/2011.03609.pdf">[pdf]</a><a href="http://arxiv.org/abs/2011.03609">[arXiv]</a></li> -->
        </ul>


      </div>

    </div>
  </div>

  <!-- Companies
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br>
    <div class="row">
      <div class="four columns center">
        <a href="https://www.cc.gatech.edu/">
          <img src="assets/icons/org/Computing.png" alt="CS department Logo" class="logo">
        </a>
        <!-- <p class="company-name">
          Research Intern @ The AI Institute<br>2024 - Present
        </p> -->
      </div>
      <div class="four columns center">
        <a href="https://www.gatech.edu/">
          <img src="assets/icons/org/GeorgiaTechLogo.png" alt="Georgia Tech Logo" class="logo">
        </a>
        <!-- <p class="company-name">
          PhD in Robotics @ Georgia Tech<br>2020 - Present
        </p> -->
      </div>
      <div class="four columns center">
        <div class="row">
          <a href="https://research.gatech.edu/robotics">
            <img src="assets/icons/org/GA-Tech-IRIM-logo.png" alt="IRIM Logo" class="logo">
          </a>
        </div>
        <!-- <p class="company-name">
          AI Residency @ Google X<br>2021 - 2022
        </p> -->
      </div>
    </div>
  </div>
  
  <!-- Latest
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Introduction/Problem Definition</u></h5>
    <p>
      The realm of Reinforcement Learning (RL) has seen remarkable advancements, particularly in the domain of video game playing agents. In this project, our focus lies specifically on Atari games, 
      with a keen emphasis on the Breakout environment. Breakout presents a classic yet challenging scenario for RL agents due to its dynamic nature and complex gameplay mechanics. By delving into this environment,
       we aim to explore novel approaches to enhance the learning capabilities of RL agents, ultimately improving their performance in challenging gaming environments.  
    </p>

     <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <!-- <div class="row video-row"> -->
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <!-- <div class="four columns">
        <div class="video-container">
          <video id="learning-robot" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/learning_robot_morph.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column"> -->
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <div class="project_title">On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and
          Learning</div> -->
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <br><em><strong>Maks Sorokin</strong>, Chuyuan Fu, Jie Tan, C. Karen Liu, Yunfei Bai, Wenlong Lu, Sehoon Ha,
          Mohi Khansari</em>
        <br><em><span class="venue">International Conference on Intelligent Robots and Systems (IROS) 2023</span></em> -->
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <p class="project_info">
          We present a learning-oriented morphology optimization framework that accounts for the interplay between the
          robot's morphology, onboard perception abilities, and their interaction in different tasks.
          We find that morphologies optimized holistically improve the robot performance by 15-20% on
          various manipulation tasks, and require 25x less data to match human-expert made morphology performance.
        </p> -->
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <a href="https://learning-robot.github.io/">[project page]</a>
        <a href="https://www.youtube.com/watch?v=w9B0COjGvfo">[video overview]</a>
        <a href="https://arxiv.org/pdf/2303.13390.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2303.13390">[arXiv]</a>
      </div>
    </div> -->

  </div>

  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Related Works</u></h5>
    <p>
      <li><b>Thananjeyan, Brijen, et al. "Recovery rl: Safe reinforcement learning with learned recovery zones." 
        IEEE Robotics and Automation Letters 6.3 (2021): 4915-4922.</b> </li>
        In the context of image-based RL, this work is particularly relevant as it 
        shows promising results on image-based navigation and manipulation tasks in simulation, 
        as well as an image-based obstacle avoidance task on a physical robot. 
        The ability to learn safety critics and recovery policies from high-dimensional
         image observations is an important capability for applying RL to real-world robotics tasks.
      <li><b>D. Yarats, et al. "Reinforcement Learning with Prototypical Representations," in Proceedings of the 38th International Conference on Machine Learning, vol. 139, pp. 11920-11931, PMLR, 2021. [Online]. Available: https://proceedings.mlr.press/v139/yarats21a.html</b></li>
      Proto-RL is a novel self-supervised framework that learns visual representations and prototypes in a task-agnostic pre-training phase for image-based reinforcement learning. 
      During pre-training, it explores the environment to maximize state entropy while learning the representations/prototypes. 
      The learned representations and prototypes then accelerate downstream task learning by providing effective embeddings and enabling better exploration, leading to state-of-the-art performance on various challenging visual control tasks from the DeepMind Control Suite.
      <li><b>D. Kalashnikov et al., "Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation," in Proceedings of The 2nd Conference on Robot Learning, vol. 87, pp. 651-673, PMLR, 2018. Available: https://proceedings.mlr.press/v87/kalashnikov18a.html
      </b></li>
      The QT-Opt algorithm presented in this paper is relevant to image-based reinforcement learning for environments like Atari Breakout. Like Atari games, QT-Opt learns policies directly from raw pixel inputs using deep Q-learning. 
      The distributed off-policy training procedure enables learning from massive amounts of experience data, which could potentially be useful for challenging exploration games like Breakout.
      Additionally, the stochastic optimization used in QT-Opt to select actions could help with the high-dimensional and potentially multimodal action spaces present in many Atari games.
      <li><b>Huang, Yangru, et al. "Spectrum random masking for generalization in image-based reinforcement learning." Advances in Neural Information Processing Systems 35 (2022): 20393-20406.</b></li>
      The paper discusses the challenge of generalization in image-based reinforcement learning (RL) and proposes a novel augmentation method called Spectrum Random Masking (SRM) to address this issue. SRM operates in the frequency domain and aims to enhance the diversity of training observations while maintaining the main content. 
      This method has been shown to improve model robustness under various distribution shifts and enhance learning stability, making it relevant to Atari games, especially Breakout environments, where the ability to generalize across different visual scenarios is crucial for effective RL performance. 
      The proposed SRM method's focus on frequency-based augmentation and its potential to improve generalization aligns with the challenges faced in training RL agents for Atari games, where visual inputs and distribution shifts are significant factors impacting performance.
      <li><b>Schulman, John, et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).</b></li>
      The paper introduces the Proximal Policy Optimization (PPO) algorithm, a family of policy gradient methods for reinforcement learning. It demonstrates the effectiveness of PPO in various benchmark tasks, including simulated robotic locomotion and Atari game playing. Specifically, the paper compares PPO with other algorithms on the Atari domain, 
      showcasing its superior performance in terms of sample complexity. In the Breakout environment, PPO demonstrates significant improvements in learning efficiency compared to other algorithms, highlighting its potential for achieving stable and reliable policy optimization in image-based reinforcement learning tasks. Overall, the paper's findings suggest 
      that PPO is a promising approach for addressing challenges in image-based reinforcement learning, particularly in Atari game environments.
    <br>
    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Methods/Approach</u></h5>
    <p>
      Our primary methodology revolves around training an RL agent using the PPO algorithm within the Atari Breakout environment. 
      The input observations provided to the agent consist of 84 x 84 grayscale images, representing the game screen at each timestep.
       To potentially enhance the agent's learning capabilities, we propose to modify these input observations using edge detection techniques. 
       By extracting edge information from the images, we aim to augment the raw visual data with more salient features, 
      potentially aiding the agent in better understanding the game dynamics and making informed decisions.
    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Experiments / Results</u></h5>
    <p>Describe what you tried and what datasets were used. We aren’t expecting you to beat state of the art, but we are interested 
      in you describing what worked or didn’t work and to give reasoning as 
      to why you believe so. Compare your approach against baselines (either previously established or you established) in this section. Provide at least one qualitative result
       (i.e. a visual output of your system on an example image). 
      <br> Note: For the project update, feel free to discuss what worked and didn’t work. Why do you think an approach was (un)successful?
       We expect you to have dealt with dataset setup and completed at least 1 experimental result by the project update.<br></p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> What’s next</u></h5>
    <p>Moving forward, our plan entails conducting a series of experiments to evaluate the impact of incorporating edge detection preprocessing on the training performance of the RL agent.
       We will meticulously analyze key performance metrics such as convergence speed, final reward attainment, and overall stability of training. 
       Additionally, we intend to explore variations in edge detection algorithms and parameters to ascertain the optimal configuration for improving agent performance. 
       Through these experiments, we strive to gain deeper insights into the interplay between image preprocessing techniques and RL training dynamics, ultimately advancing the state-of-the-art in gaming agent development. </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Team member contributions</u></h5>
    <p>Indicate what you anticipate each team member will contribute by the final project submission.</p>
    
    <br>Note: List every member name and their corresponding tasks in bullet points – or you may 
    simply assign team member names to the task list you created above. 
    <br>
    <li>Juntao He</li>
    Build the project website and write the ralated works part.
  </div>
  
  <!-- Teaching ––––––––––––––––––––––––––––––––––––––––––––––––––
  <div class="container">
    <div class="div_line"></div><br><br>
    <div class="row profile-row">
      <div class="one columns hide-on-small">&nbsp;</div>
      <div class="nine columns ">
        <h5><u>Mentoring Experience</u></h5>
        I've had a great pleasure working with a number of exceptional students at Georgia Tech.<br><br>
        <ul>
          <li><strong>PRESENT</strong>: Master's Student - <a href="https://jxu443.github.io/portfolio/">Jiaxi Xu</a></li>
          <li><strong>FALL 2021</strong>: Master's Student - <a href="https://arjun-krishna.github.io/">Arjun Krishna</a> -> PhD student at UPenn’s GRASP lab </li>
          <li><strong>FALL 2020</strong>: Master's Student - <a href="https://qianluo.netlify.app">Qian Luo</a> ->
            NLP Algorithm Engineer at Alibaba Group</li>
        </ul>
        <br>
        <h5><u>Teaching Experience</u></h5>
        I had an amazing experience helping teach one of the largest classes (1000+ students) at Georgia Tech.
        <br>CS6601 - Artificial Intelligence class by
        <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Dr. Thomas Ploetz</a> & <a
          href="https://www.cc.gatech.edu/home/thad/">Dr. Thad Starner</a>.
        <br><br>
        <ul>
          <li><strong>FALL 2019 & SPRING 2020</strong>: Head Teaching Assistant </li>
          <li><strong>FALL 2018 & SPRING 2019</strong>: Teaching Assistant </li>
        </ul>
        <br>
        <h5><u>Scholarly Activities</u></h5>
        <ul>
          <li><strong>IROS 2023</strong> - Session co-chair Mechanism Design </li>
          <li><strong>RA-L 2023</strong> - Reviewer at IEEE Robotics and Automation Letters </li>
          <li><strong>RSS 2023</strong> - Reviewer at Proceedings of Robotics: Science and Systems </li>
          <li><strong>RA-L 2022</strong> - Reviewer at IEEE Robotics and Automation Letters</li>
          <li><strong>ICRA 2021</strong> - Reviewer at IEEE International Conference on Robotics and Automation</li>
        </ul>
      </div>
    </div>
  </div> -->


  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- attempt hiding from spambots :p - snippet credits Andrej Karpathy - karpathy.ai source code -->
  <script type="text/javascript">
    var e_is_shown = false;
    document.getElementById('iemail').addEventListener("click", function () {
      let demail = document.getElementById('demail');
      demail.innerHTML = 'm' + 'aks' + ' _a' + 't_ ' + 'gatech' + '.' + 'e' + 'du';
      demail.style.opacity = e_is_shown ? 0 : 1;
      e_is_shown = !e_is_shown;
    })
  </script>


  <!-- ### FOOTER ### -->

  <!-- <br>
  <br>

  <br>
  <br>
  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    <h7 style="text-decoration: none;">consider checking out: </h7><br><br>
    <a href="https://www.givewell.org/"><img src="assets/icons/gw.jpg" style="width:100px;"></a>
    <a href="https://www.givingwhatwecan.org/"><img src="assets/icons/gwwc.png" style="width:100px;"></a>
    <a href="https://www.effectivealtruism.org/"><img src="assets/icons/ea.png" style="width:80px; margin:10px"></a>

  </p>


  <div class="div_line"></div><br> -->

  <!-- <p style="color:gray; text-align:center; font-size: small;">
    2023© Maks Sorokin<br>
    built using <a href="http://getskeleton.com/">Skeleton</a>,
    icon credits <a href="https://www.flaticon.com/"> flaticon</a>,
    hosted by <a href="https://pages.github.com/"> GitHub Pages</a>❤️
    <br>
    <br>
    feel free to copy: <a href="https://github.com/initmaks/initmaks.github.io">this page</a>
  </p> -->

</body>

</html>