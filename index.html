<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Gatech_CV_Cacao</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="assets/"> -->

  <!-- Video playback speed and no controls -->
  <script defer src="js/video.js"></script>
</head>

<body>
  <div class="container">


        </br>

        <h3 style="text-align: center;">Bridging from images to actions : </br>How computer vision techniques affect RL agent? </h3>
        <h5 style="text-align: center;">Georgia Tech CS6476 Project (Mar 2024)  </br>Eva Li, Juntao He, and Seungeun Rho</h5>


    </div>


  <!-- Companies
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<!--  <div class="container">-->
<!--    <div class="div_line"></div><br>-->
<!--    <div class="row">-->
<!--      <div class="four columns center">-->
<!--        <a href="https://www.cc.gatech.edu/">-->
<!--          <img src="assets/icons/org/Computing.png" alt="CS department Logo" class="logo">-->
<!--        </a>-->
<!--        &lt;!&ndash; <p class="company-name">-->
<!--          Research Intern @ The AI Institute<br>2024 - Present-->
<!--        </p> &ndash;&gt;-->
<!--      </div>-->
<!--      <div class="four columns center">-->
<!--        <a href="https://www.gatech.edu/">-->
<!--          <img src="assets/icons/org/GeorgiaTechLogo.png" alt="Georgia Tech Logo" class="logo">-->
<!--        </a>-->
<!--        &lt;!&ndash; <p class="company-name">-->
<!--          PhD in Robotics @ Georgia Tech<br>2020 - Present-->
<!--        </p> &ndash;&gt;-->
<!--      </div>-->
<!--      <div class="four columns center">-->
<!--        <div class="row">-->
<!--          <a href="https://research.gatech.edu/robotics">-->
<!--            <img src="assets/icons/org/GA-Tech-IRIM-logo.png" alt="IRIM Logo" class="logo">-->
<!--          </a>-->
<!--        </div>-->
<!--        &lt;!&ndash; <p class="company-name">-->
<!--          AI Residency @ Google X<br>2021 - 2022-->
<!--        </p> &ndash;&gt;-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--  -->
  <!-- Latest
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Project description </u></h5>

    <figure>

      <img src="assets/img/fig1.png" alt=" fig1" width="200" class="center">
      <figcaption class="center">Figure 1. An in-game scene of Atari-Breakout</figcaption>

    </figure>

    <p>
      In this project, we aim to investigate how both classical and modern techniques in computer vision affects to the performance of
      Reinforcement Learning (RL) agents in game domains. We trained an agent that decides an action based on an observation using RL.
      We used Breakout-Atrai game as our benchmark environments, since Breakout presents a classic yet challenging scenario for RL agents due to its dynamic nature and complex gameplay mechanics.
      For each step, the only information given to the agent is RGB image, so the agent should be able to infer the true state by encoding the low-level image input
      into high-level embeddings, which later will be used to decide actions.</p>
    <p>
      To demonstrate the effectiveness of computer vision techniques we learned from class, we apply following two different techniques for extracting the informations from image,
      and see how the final performance changes.
      </br></br>

      <b>1) Enhancing observation using Edge Detectors</b></br>
      By augmenting the raw visual data with edge information, we aim to provide the RL agent with more salient features to better understand the game dynamics.
      Our primary goal is to evaluate whether incorporating edge detection preprocessing enhances the training performance of the RL agent, ultimately leading to
      improved gameplay proficiency.
      </br></br>
      <b>2) Ablation study of neural network architectures - CNN vs Vision Transformer</b></br>
      During the coursework we learned two most prominent neural network archiectures for image encoding; Convolutional Neural Networks(CNN) and Vision Transformer(ViT).
      We aim to compare the effectiveness of each architecture for encoding image for RL agents.
      </br></br>

      Since the RL has seen remarkable advancements particularly in the domain of video game playing agents, we believe that understanding the usefulness of each computer vision
      techniques for encoding images will lead to deeper understanding of each method.


    </p>

    <p>

    </p>

     <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <!-- <div class="row video-row"> -->
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <!-- <div class="four columns">
        <div class="video-container">
          <video id="learning-robot" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/learning_robot_morph.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column"> -->
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <div class="project_title">On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and
          Learning</div> -->
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <br><em><strong>Maks Sorokin</strong>, Chuyuan Fu, Jie Tan, C. Karen Liu, Yunfei Bai, Wenlong Lu, Sehoon Ha,
          Mohi Khansari</em>
        <br><em><span class="venue">International Conference on Intelligent Robots and Systems (IROS) 2023</span></em> -->
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <p class="project_info">
          We present a learning-oriented morphology optimization framework that accounts for the interplay between the
          robot's morphology, onboard perception abilities, and their interaction in different tasks.
          We find that morphologies optimized holistically improve the robot performance by 15-20% on
          various manipulation tasks, and require 25x less data to match human-expert made morphology performance.
        </p> -->
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <!-- <a href="https://learning-robot.github.io/">[project page]</a>
        <a href="https://www.youtube.com/watch?v=w9B0COjGvfo">[video overview]</a>
        <a href="https://arxiv.org/pdf/2303.13390.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2303.13390">[arXiv]</a>
      </div>
    </div> -->

  </div>

  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Related Works</u></h5>
    In this section, we cover representative works from the domain of image based deep RL.
    <p>
    <li><b>Schulman, John, et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).</b></li>
      The paper introduces the Proximal Policy Optimization (PPO) algorithm, a family of policy gradient methods for reinforcement learning.
      To train and evaluate the performance for each of methods, we used this algorithm. This paper demonstrates the effectiveness of PPO algorithm in various
      benchmark tasks, including simulated robotic locomotion and Atari game playing. Specifically, the paper compares PPO with other algorithms on the Atari domain,
      showcasing its superior performance in terms of sample complexity. In the Breakout environment, PPO demonstrates significant improvements in learning
      efficiency compared to other algorithms, highlighting its potential for achieving stable and reliable policy optimization in image-based reinforcement learning
       tasks.

    </br></br>
    <li><b>Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).</b></li>
    This paper presents the use of transformer for encoding images. Before this paper came out, transformer was mostly used for encoding sequential data.
    This paper suggests that you can translate image into smaller patches and flatten in 1D to make it look like sequential data, and using transformer
    on top of it gives us efficient embeddings of image.
    </br></br>
    <li><b>D. Kalashnikov et al., "Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation," in Proceedings of The 2nd Conference on Robot
      Learning, vol. 87, pp. 651-673, PMLR, 2018 </b></li>
      The QT-Opt algorithm presented in this paper is relevant to image-based reinforcement learning for environments like Atari Breakout.
      Like Atari games, QT-Opt learns policies directly from raw pixel inputs using deep Q-learning.
      The distributed off-policy training procedure enables learning from massive amounts of experience data, which could potentially be useful for increasing the sample efficiency
      for gaming enviromnets such as Atari.
      Additionally, the stochastic optimization used in QT-Opt to select actions could help with the high-dimensional and potentially multimodal action spaces present
      in many Atari games.
    </br></br>

      <li><b>D. Yarats, et al. "Reinforcement Learning with Prototypical Representations," in Proceedings of the 38th International Conference on Machine Learning,
        vol. 139, pp. 11920-11931, PMLR, 2021. </b></li>
      Proto-RL is a novel self-supervised framework that learns visual representations and prototypes in a task-agnostic pre-training phase for image-based reinforcement learning. 
      During pre-training, it explores the environment to maximize state entropy while learning the representations/prototypes. 
      The learned representations and prototypes then accelerate downstream task learning by providing effective embeddings and enabling better exploration, leading to state-of-the-art performance on various challenging visual control tasks from the DeepMind Control Suite.
    </br></br>

    <li><b>Huang, Yangru, et al. "Spectrum random masking for generalization in image-based reinforcement learning." Advances in Neural Information Processing
      Systems 35 (2022): 20393-20406.</b></li>
      The paper discusses the challenge of generalization in image-based reinforcement learning (RL) and proposes a novel augmentation method called Spectrum Random Masking (SRM) to address this issue. SRM operates in the frequency domain and aims to enhance the diversity of training observations while maintaining the main content. 
      This method has been shown to improve model robustness under various distribution shifts and enhance learning stability, making it relevant to Atari games, especially Breakout environments, where the ability to generalize across different visual scenarios is crucial for effective RL performance. 
      The proposed SRM method's focus on frequency-based augmentation and its potential to improve generalization aligns with the challenges faced in training RL agents for Atari games, where visual inputs and distribution shifts are significant factors impacting performance.


    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Methods/Approach</u></h5>
    <p>
    Our objective is to train an RL agent using the Proximal Policy Optimization (PPO) algorithm, and see how the performance varies depending on differnet types of
    Computer Vision(CV) techniques. We are planning to apply two types of techniques. The first techniques is classical; we are going to apply edge detector
    for enhancing raw observation, and see whether it gives more avid signals when the scene is changed during the game. The second techniques is investigate
      different neural network architecture. More specifically, we are going to test both CNN and ViT. However for this report, we only cover the first technique
      and leave the second one for the final report.


    </p>



    <div class="eleven columns ">
      <p><b>1) Enhancing observation using Edge Detectors </b></p>
      <figure>
          <img src="assets/img/fig7.png" alt=" fig1" width="1000" class="center">
          <figcaption class="center">Figure 2. Original Frame</figcaption>
        </figure>
        <p>A pivotal aspect of our methodology is the application of targeted image preprocessing techniques aimed at optimizing the visual information fed into
          the model.To optimize the agent's ability to learn from visual inputs, we developed a preprocessing pipeline that includes the following
          steps. First, the original input observations from the game is RGB image with size of 210 x 160 (Figure 1.a). We first <b>preprocess</b> this image and then applies
          edge detectors on top of it. Figure 2 illustrates the examplary image from each procedure
          <ul>
            <li>(a) Original Frame</li>
            <li>(b) After Preprocessing</li>
              <ul>
                  <li>resize into 84 x 84</li>
                  <li>transform into gray image</li>
                  <li>cut out the top 16 pixels and bottom 4 pixels which are irrelevant to game playing</li>
                  <li>stack 4 consecutive images :  providing the agent with temporal context necessary for understanding motion and progression within the game.</li>
                </ul>
            <li>(c) Edge Enhanced Frame After Preprocessing </li>
            <li>(d) Edge Enhanced + Color Scale Frame After Preprocessing</li>
          </ul>



    </div>
    <div class="eleven columns ">
    <p><b>2) Reinforcement Learning </b></p>
    We directly implemented RL algorihtm, Proximal Policy Optimization (PPO) by ourselves to avoid the overhead of more complex systems. This enhanced computational
      efficiency and facilitates experimentation.

    <ul>
      <li><strong>Convolutional Neural Network (CNN):</strong> Our model architecture includes a CNN tailored for extracting features from game frames. The network consists of three convolutional layers, each followed by ReLU activation and max pooling, culminating in a flattened layer for further processing.</li>
      <li><strong>Policy and Value Networks:</strong> Separate networks derive from the CNN's output, with the policy network predicting action probabilities and the value network estimating state values. This separation facilitates the efficient optimization of both policy and value predictions.</li>
    </ul>

    <p>Our training process is governed by a carefully selected set of hyperparameters, optimized to balance learning efficiency and performance:</p>

    <ul>
      <li><strong>Learning Rate:</strong> 0.0001, controls the step size during optimization, ensuring steady convergence without overshooting.</li>
      <li><strong>Gamma (γ):</strong> 0.98, discounts future rewards, balancing immediate and long-term gains.</li>
      <li><strong>Lambda (λ):</strong> 0.95, used in the Generalized Advantage Estimation (GAE), influencing the bias-variance trade-off in advantage computation.</li>
      <li><strong>Epsilon Clip (ε):</strong> 0.1, limits the ratio of new to old policy probabilities, preventing large policy updates that could destabilize training.</li>
      <li><strong>K Epochs:</strong> 3, the number of epochs for updating the policy using the same set of sampled data, ensuring thorough utilization of each batch.</li>
      <li><strong>T Horizon:</strong> 20, defines the length of the trajectory segment used for each update, balancing the trade-off between update frequency and the quality of the advantage estimates.</li>
    </ul>

<p><strong>Custom Vector Environment</strong></p>

<p>We also implemented designed algorithm in vectorized environment so that multiple instances of the environment can be run in parallel. This means that the agent can collect more diverse experience in a shorter amount of time, leading to faster learning and improved overall performance.</p>

</div>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> Experiments / Results</u></h5>
    <p>We conducted three distinct training experiments on the Breakout game, employing different image preprocessing strategies to evaluate their impact on learning performance (Figure 3).</p>

<ol>
  <li>
    <strong>Default Preprocessing (Blue):</strong> Our first experiment adhered to the default preprocessing settings provided by OpenAI's AtariPreprocessing, FrameStacking, and simple crop of top and bottom of the page that consist of useless information for agent training. This method provided a baseline for performance comparison, focusing on the original visual representation with minimal modifications. Shows a score ending near 40.
  </li>
  <li>
    <strong>Edge Detection Only (Black):</strong> The second approach used edge detection exclusively to process the game images. This technique, while effective in highlighting structural information, resulted in suboptimal performance due to the absence of color data, which is essential for recognizing score-related changes in the game environment. Shows a score increase modestly to just above 20.
  </li>
  <li>
    <strong>Edge Detection with Image Stacking (Pink):</strong> The most successful approach combined edge detection with image stacking, integrating both structural and temporal information into the AI's decision-making process with scores improving significantly to reach around 60. This method significantly outperformed the others, indicating that a composite representation of visual data, incorporating both immediate and historical game states, yields superior learning outcomes.
  </li>
</ol>
<style>  .custom-figure {
  width: 800px; /* Adjust width as needed */
  height: auto; /* Adjust height as needed, or use a fixed height */
  float: left;
  margin-right: 20px; /* Optional: Add some spacing between the figure and other content */
}
  /* Additional CSS for demonstration purposes */
  .custom-figure img {
    max-width: 100%; /* Ensure the image doesn't exceed the figure width */
    height: auto; /* Maintain aspect ratio */
    display: block; /* Ensure proper alignment */
    margin: 0 auto; /* Center the image horizontally */
  }

</style>

<figure >
  <img src="assets/img/fig5_new.png" alt=" fig5" class="center" width="1000">
  <figcaption class="center">Fig 3. Score Change of Three Preprocessing Procedures Over 45k Training Iterations</figcaption>
</figure>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u> What’s next</u></h5>
    <p>
    Implementation of ViT (~April 10th) : Juntao He, Eva Li </br>
      Experimentation with ViT (~April 17th): Seungeun Rho</br>
      Final Report (~April 20th): Seungeun Rho, Juntao He, Eva Li</br>
    </p>
  </div>
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Team member contributions</u></h5>
    <li>Eva Li : In charge of experimentations. Implemented Edge detector. Tested different types of methods and report results.</li>

    <li>Seungeun Rho : In charge of implementations. Implemented backbone PPO codes, custom Vectorized environments, agent architectures, and training code.</li>

    <li>Juntao He : In charge of project webpage and report. Related work research. Also helped implementing features from backbone codes.</li>

  </div>
  
  <!-- Teaching ––––––––––––––––––––––––––––––––––––––––––––––––––
  <div class="container">
    <div class="div_line"></div><br><br>
    <div class="row profile-row">
      <div class="one columns hide-on-small">&nbsp;</div>
      <div class="nine columns ">
        <h5><u>Mentoring Experience</u></h5>
        I've had a great pleasure working with a number of exceptional students at Georgia Tech.<br><br>
        <ul>
          <li><strong>PRESENT</strong>: Master's Student - <a href="https://jxu443.github.io/portfolio/">Jiaxi Xu</a></li>
          <li><strong>FALL 2021</strong>: Master's Student - <a href="https://arjun-krishna.github.io/">Arjun Krishna</a> -> PhD student at UPenn’s GRASP lab </li>
          <li><strong>FALL 2020</strong>: Master's Student - <a href="https://qianluo.netlify.app">Qian Luo</a> ->
            NLP Algorithm Engineer at Alibaba Group</li>
        </ul>
        <br>
        <h5><u>Teaching Experience</u></h5>
        I had an amazing experience helping teach one of the largest classes (1000+ students) at Georgia Tech.
        <br>CS6601 - Artificial Intelligence class by
        <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Dr. Thomas Ploetz</a> & <a
          href="https://www.cc.gatech.edu/home/thad/">Dr. Thad Starner</a>.
        <br><br>
        <ul>
          <li><strong>FALL 2019 & SPRING 2020</strong>: Head Teaching Assistant </li>
          <li><strong>FALL 2018 & SPRING 2019</strong>: Teaching Assistant </li>
        </ul>
        <br>
        <h5><u>Scholarly Activities</u></h5>
        <ul>
          <li><strong>IROS 2023</strong> - Session co-chair Mechanism Design </li>
          <li><strong>RA-L 2023</strong> - Reviewer at IEEE Robotics and Automation Letters </li>
          <li><strong>RSS 2023</strong> - Reviewer at Proceedings of Robotics: Science and Systems </li>
          <li><strong>RA-L 2022</strong> - Reviewer at IEEE Robotics and Automation Letters</li>
          <li><strong>ICRA 2021</strong> - Reviewer at IEEE International Conference on Robotics and Automation</li>
        </ul>
      </div>
    </div>
  </div> -->


  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- attempt hiding from spambots :p - snippet credits Andrej Karpathy - karpathy.ai source code -->
  <script type="text/javascript">
    var e_is_shown = false;
    document.getElementById('iemail').addEventListener("click", function () {
      let demail = document.getElementById('demail');
      demail.innerHTML = 'm' + 'aks' + ' _a' + 't_ ' + 'gatech' + '.' + 'e' + 'du';
      demail.style.opacity = e_is_shown ? 0 : 1;
      e_is_shown = !e_is_shown;
    })
  </script>


  <!-- ### FOOTER ### -->

  <!-- <br>
  <br>

  <br>
  <br>
  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    <h7 style="text-decoration: none;">consider checking out: </h7><br><br>
    <a href="https://www.givewell.org/"><img src="assets/icons/gw.jpg" style="width:100px;"></a>
    <a href="https://www.givingwhatwecan.org/"><img src="assets/icons/gwwc.png" style="width:100px;"></a>
    <a href="https://www.effectivealtruism.org/"><img src="assets/icons/ea.png" style="width:80px; margin:10px"></a>

  </p>


  <div class="div_line"></div><br> -->

  <!-- <p style="color:gray; text-align:center; font-size: small;">
    2023© Maks Sorokin<br>
    built using <a href="http://getskeleton.com/">Skeleton</a>,
    icon credits <a href="https://www.flaticon.com/"> flaticon</a>,
    hosted by <a href="https://pages.github.com/"> GitHub Pages</a>❤️
    <br>
    <br>
    feel free to copy: <a href="https://github.com/initmaks/initmaks.github.io">this page</a>
  </p> -->

</body>

</html>